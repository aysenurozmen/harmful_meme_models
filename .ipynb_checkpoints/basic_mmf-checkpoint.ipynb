{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba500ad-b740-4318-8bca-272e29cbc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision.transforms.functional import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf310d8-c3ec-485b-9344-cce1745684ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Image transformation\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # pads or shrinks the image to 224*224\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3532980a-eb24-48aa-b856-57cdf9a082d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMF dataset class\n",
    "class MmfDataset(Dataset):\n",
    "    def __init__(self, data, image_folder, image_transform, tokenizer):\n",
    "        self.data = data\n",
    "        self.image_folder = image_folder\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "\n",
    "        # Load and preprocess image\n",
    "        image_path = self.image_folder + entry[\"image\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        # Tokenize and obtain text embeddings using BERT\n",
    "        text = entry[\"text\"]\n",
    "        tokens = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            text_embedding = bert_model(**tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Label encoding\n",
    "        label = entry[\"labels\"][0]\n",
    "        if label == \"not harmful\":\n",
    "            encoded_label = 0\n",
    "        elif label == \"somewhat harmful\":\n",
    "            encoded_label = 1\n",
    "        elif label == \"very harmful\":\n",
    "            encoded_label = 2\n",
    "\n",
    "        # Convert encoded_label to a PyTorch tensor\n",
    "        encoded_label_tensor = torch.tensor(encoded_label)\n",
    "\n",
    "        return image, text_embedding, encoded_label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cf5d7-76b3-473a-8cca-529ba91a56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unpack the batch into separate lists for images, text_embeddings, and labels\n",
    "    images, text_embeddings, labels = zip(*batch)\n",
    "\n",
    "    # Stack images and text_embeddings into tensors\n",
    "    images = torch.stack(images)\n",
    "    text_embeddings = torch.stack(text_embeddings)\n",
    "\n",
    "    # Stack labels into a tensor\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return images, text_embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b94116-d97e-4893-a767-6977f4aed9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\aysen\\\\Documents\\\\GitHub\\\\harmful_meme_models\\\\data\\\\datasets\\\\memes\\\\defaults\\\\annotations\\\\train.jsonl\"\n",
    "image_folder = \"C:\\\\Users\\\\aysen\\\\Documents\\\\GitHub\\\\harmful_meme_models\\\\data\\\\datasets\\\\memes\\\\defaults\\\\images\\\\\"\n",
    "\n",
    "# Read the JSON string from the file\n",
    "with open(dataset_path, \"r\", encoding='cp437') as file:\n",
    "    dataset_str = file.read()\n",
    "    file.close()\n",
    "\n",
    "# Parse the JSON string\n",
    "dataset = [json.loads(entry) for entry in dataset_str.strip().split('\\n')]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader instances for both training and validation sets\n",
    "mmf_dataset_train = MmfDataset(data=train_dataset, image_folder=image_folder, image_transform=image_transform, tokenizer=tokenizer)\n",
    "data_loader_train = DataLoader(mmf_dataset_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "mmf_dataset_val = MmfDataset(data=val_dataset, image_folder=image_folder, image_transform=image_transform, tokenizer=tokenizer)\n",
    "data_loader_val = DataLoader(mmf_dataset_val, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45677b6-b365-400c-b40d-55aa045d2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MmfClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_feature_size, text_feature_size, num_classes):\n",
    "        \n",
    "        super(MmfClassifier, self).__init__()\n",
    "        self.shared_layer = nn.Linear(image_feature_size + text_feature_size, 256)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.output_layer = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, image_data, text_data):\n",
    "\n",
    "        # Reshape to (batch_size, channels*height*width)\n",
    "        flattened_image_data = image_data.view(image_data.size(0), -1)\n",
    "\n",
    "        # Reshape to (batch_size, sequence_length*embedding_size)\n",
    "        flattened_text_data = text_data.view(text_data.size(0), -1)\n",
    "\n",
    "        # Combine visual and textual features \n",
    "        combined_features = torch.cat((flattened_image_data, flattened_text_data), dim=1)\n",
    "        shared_output = self.relu(self.shared_layer(combined_features))\n",
    "        output = self.output_layer(shared_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829c4a5-a330-4333-aa6e-55d86d6657a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_feature_size = 3*224*224 # Image feature size\n",
    "text_feature_size = 768  # Text feature size\n",
    "num_classes = 3  # Number of classes\n",
    "\n",
    "# Instantiate the model\n",
    "model = MmfClassifier(image_feature_size, text_feature_size, num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d504ae5-7f6e-4312-a25f-3084f544e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First entry in the dataset:\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff50e6-a570-4f53-ac09-0a019cd04a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation loops\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in data_loader_train:\n",
    "        images, text_embeddings, labels = batch\n",
    "        outputs = model(images, text_embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_val in data_loader_val:\n",
    "            images_val, text_embeddings_val, labels_val = batch_val\n",
    "            outputs_val = model(images_val, text_embeddings_val)\n",
    "            loss_val = criterion(outputs_val, labels_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs_val, 1)\n",
    "            correct_predictions += (predicted == labels_val).sum().item()\n",
    "            total_samples += labels_val.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(data_loader_val)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
